{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f8bed4",
   "metadata": {},
   "source": [
    "# Hugging Face Hub - Setup by Downloading Models\n",
    "\n",
    "## !! This notebook is not meant to be run, just to explain the setup !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d53b1d",
   "metadata": {},
   "source": [
    "## Teaching LLM workflow by using open source models and Hugging Face Hub\n",
    "\n",
    "The **Hugging Face Hub** is a platform that hosts thousands of pre-trained models, datasets, and demos. It's the go-to source for downloading quantized GGUF models that can run efficiently on CPU.\n",
    "\n",
    "This info is as of the writing of this notebook in December 2025 and this info is changing rapidly.\n",
    "\n",
    "### Why Hugging Face Hub?\n",
    "\n",
    "- **Vast model selection**: Access to thousands of GGUF models, not limited to a curated subset\n",
    "- **Standard format**: All models use the GGUF format compatible with llama.cpp and llama-cpp-python\n",
    "- **Configurable caching**: Control where models are downloaded and stored\n",
    "- **Simple Python API**: Easy-to-use `hf_hub_download` function\n",
    "\n",
    "### Shared Filesystem\n",
    "\n",
    "In the setup where I was teaching, I used this notebook to download models from Hugging Face and I put them in a shared-readwrite folder where the students could access them on JupyterHub. This was possible because I was using a JupyterHub for teaching that had a shared folder system.\n",
    "\n",
    "Your use case may vary. It could look like...\n",
    "- Shared read-write directory on JupyterHub\n",
    "- Each student downloads their own models\n",
    "- Download models to local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4031df85-3599-4e95-a59d-f476cb36decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that your python environment has huggingface_hub package installed.\n",
    "try:\n",
    "    from huggingface_hub import hf_hub_download\n",
    "except ImportError:\n",
    "    %pip install huggingface_hub\n",
    "    from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d330d",
   "metadata": {},
   "source": [
    "## Which model to download\n",
    "\n",
    "In the use case for teaching on a JupyterHub with a CPU, I was looking for **small models**:\n",
    "- ~1-2 billion parameters\n",
    "- Quantized (Weights have 4 decimal places instead of 10)\n",
    "\n",
    "You can explore the world of models at:\n",
    "[Hugging Face Model List](https://huggingface.co/models?search=gguf)\n",
    "\n",
    "When searching for GGUF models, look for:\n",
    "- **Q4_K_M** or **Q4_0**: 4-bit quantization, good balance of size and quality\n",
    "- **Q5_K_M**: Slightly larger but better quality\n",
    "- **Q8_0**: 8-bit quantization, best quality but larger file size\n",
    "\n",
    "### Recommended small models for teaching:\n",
    "\n",
    "| Model | Repo ID | Filename | Size |\n",
    "|-------|---------|----------|------|\n",
    "| TinyLlama 1.1B | `TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF` | `tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf` | ~670 MB |\n",
    "| Qwen2 1.5B | `Qwen/Qwen2-1.5B-Instruct-GGUF` | `qwen2-1_5b-instruct-q4_0.gguf` | ~900 MB |\n",
    "| Llama 3.2 1B | `bartowski/Llama-3.2-1B-Instruct-GGUF` | `Llama-3.2-1B-Instruct-Q4_K_M.gguf` | ~700 MB |\n",
    "| Phi-3 Mini | `bartowski/Phi-3-mini-4k-instruct-GGUF` | `Phi-3-mini-4k-instruct-Q4_K_M.gguf` | ~2.3 GB |\n",
    "| DeepSeek R1 1.5B | `bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF` | `DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf` | ~1 GB |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f784d27-ea55-4a2f-86e7-d08ff0e5f2f1",
   "metadata": {},
   "source": [
    "## Let's check out our local filesystem path and where we will download the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005c223-3f01-4478-92d6-3c2de6bb1a9d",
   "metadata": {},
   "source": [
    "### Approach 1 - If a Shared Hub is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054e688-fa14-4990-aa56-be9aea92614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cal-ICOR workshop Hub specific path\n",
    "!ls /home/jovyan/shared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595269ab-c7c0-4da5-8110-2907dfc36d86",
   "metadata": {},
   "source": [
    "### Approach 2 - If a local machine is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600fff6e-0a22-4d6f-ae3e-8c4c28d6d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is my local path to a directory called shared-rw\n",
    "!ls ../shared/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f0889-5886-40ad-bd60-df4745851d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or the full path (this is on my laptop)\n",
    "!ls /Users/ericvandusen/Documents/GitHub/shared/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c238eb73-62e8-4cee-aeb3-34a81be11218",
   "metadata": {},
   "source": [
    "### Set the path where the models will download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ce40c-662d-4815-8339-0da6b45c59ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for Shared Hub - change this to match your JupyterHub's shared directory\n",
    "# Examples: /home/jovyan/shared, /home/jovyan/shared_readwrite, /home/jovyan/_shared/course-name\n",
    "shared_model_path = \"/home/jovyan/shared\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d6020-693b-4bef-8810-de0d21675bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for Local\n",
    "shared_model_path = \"/Users/ericvandusen/Documents/GitHub/shared/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d72df-194d-4209-b728-2ae92f5273f6",
   "metadata": {},
   "source": [
    "## Downloading Models with Hugging Face Hub\n",
    "\n",
    "The `hf_hub_download` function downloads a specific file from a Hugging Face repository.\n",
    "\n",
    "### Key Parameters:\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `repo_id` | The repository identifier (e.g., `\"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"`) |\n",
    "| `filename` | The specific file to download (e.g., `\"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"`) |\n",
    "| `local_dir` | Directory where the file will be stored (for shared access) |\n",
    "| `local_dir_use_symlinks` | Set to `False` to copy files instead of creating symlinks |\n",
    "\n",
    "### Default Behavior vs Shared Repository\n",
    "\n",
    "By default, `hf_hub_download` stores files in `~/.cache/huggingface/hub/`, which is user-specific. To enable shared access for students, we use the `local_dir` parameter to specify a shared directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8a73d-title",
   "metadata": {},
   "source": [
    "### Download TinyLlama 1.1B (Recommended for teaching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8a73d-e34f-4c52-bd7f-5e70f0b869d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download TinyLlama model to shared directory\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\",\n",
    "    filename=\"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\",\n",
    "    local_dir=shared_model_path,\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "print(f\"Model downloaded to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e739378-title",
   "metadata": {},
   "source": [
    "### Download Qwen2 1.5B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e739378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Qwen2 model to shared directory\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"Qwen/Qwen2-1.5B-Instruct-GGUF\",\n",
    "    filename=\"qwen2-1_5b-instruct-q4_0.gguf\",\n",
    "    local_dir=shared_model_path,\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "print(f\"Model downloaded to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e82d4a-title",
   "metadata": {},
   "source": [
    "### Download Llama 3.2 1B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e82d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Llama 3.2 model to shared directory\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/Llama-3.2-1B-Instruct-GGUF\",\n",
    "    filename=\"Llama-3.2-1B-Instruct-Q4_K_M.gguf\",\n",
    "    local_dir=shared_model_path,\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "print(f\"Model downloaded to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef346506-title",
   "metadata": {},
   "source": [
    "### Download DeepSeek R1 Distill 1.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef346506-e0ee-4f13-be1f-a2090850f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download DeepSeek model to shared directory\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF\",\n",
    "    filename=\"DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf\",\n",
    "    local_dir=shared_model_path,\n",
    "    local_dir_use_symlinks=False\n",
    ")\n",
    "\n",
    "print(f\"Model downloaded to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1725bb95-4ed2-4a32-947a-279c751e3447",
   "metadata": {},
   "source": [
    "## Let's now check which models we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37c9e6-936a-40be-b9a5-a213552b1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh \"{shared_model_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2466141d-0062-4f9a-a26e-987f8f5f58c0",
   "metadata": {},
   "source": [
    "## Testing the Downloaded Model with llama-cpp-python\n",
    "\n",
    "Let's verify that our downloaded model works correctly by loading it with llama-cpp-python and generating a simple response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure llama-cpp-python is installed\n",
    "try:\n",
    "    from llama_cpp import Llama\n",
    "except ImportError:\n",
    "    %pip install llama-cpp-python\n",
    "    from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Path to our downloaded model\n",
    "model_file = os.path.join(shared_model_path, \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\")\n",
    "\n",
    "# Load the model\n",
    "print(f\"Loading model from: {model_file}\")\n",
    "llm = Llama(\n",
    "    model_path=model_file,\n",
    "    n_ctx=2048,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-generate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation\n",
    "response = llm.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Can you tell me a fun fact about llamas?\"}\n",
    "    ],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856c4d5",
   "metadata": {},
   "source": [
    "## Bonus: Searching for Models on Hugging Face\n",
    "\n",
    "You can also use the Hugging Face Hub API to search for models programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06680376-15c4-4cf3-aecb-c802e474352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, list_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a86bc-c0a5-461b-bb3a-5f1a33d89398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for GGUF models\n",
    "api = HfApi()\n",
    "\n",
    "# Find models with \"gguf\" in the name, sorted by downloads\n",
    "models = list(api.list_models(\n",
    "    search=\"gguf\",\n",
    "    sort=\"downloads\",\n",
    "    limit=20\n",
    "))\n",
    "\n",
    "print(\"Top 20 GGUF models by downloads:\")\n",
    "print(\"-\" * 60)\n",
    "for model in models:\n",
    "    print(f\"{model.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in a specific repository to find available quantizations\n",
    "from huggingface_hub import list_repo_files\n",
    "\n",
    "repo_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n",
    "files = list_repo_files(repo_id)\n",
    "\n",
    "print(f\"Files in {repo_id}:\")\n",
    "print(\"-\" * 60)\n",
    "for f in files:\n",
    "    if f.endswith(\".gguf\"):\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. **Install** the `huggingface_hub` package\n",
    "2. **Download GGUF models** using `hf_hub_download`\n",
    "3. **Configure shared storage** for classroom environments\n",
    "4. **Test downloaded models** with llama-cpp-python\n",
    "5. **Search for models** on Hugging Face programmatically\n",
    "\n",
    "### Key Advantages of Hugging Face Hub:\n",
    "\n",
    "- **Huge model selection**: Thousands of GGUF models available\n",
    "- **Configurable caching**: Easy to set up shared directories for classrooms\n",
    "- **Automatic versioning**: Models are versioned and can be pinned to specific commits\n",
    "- **Simple API**: Just two parameters needed: `repo_id` and `filename`\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- See `LlamaCpp_SmallLM_Demo.ipynb` for detailed usage of downloaded models\n",
    "- Explore different quantization levels (Q4, Q5, Q8) for your use case\n",
    "- Try models from different families (Llama, Qwen, Phi, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
