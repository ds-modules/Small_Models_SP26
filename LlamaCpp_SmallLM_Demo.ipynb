{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Intro to AI workflows using Small LM from Huggingface\n",
    "## Using llama-cpp-python Package\n",
    "\n",
    "This notebook is adapted from the [GPT4All_SmallLM_Demo.ipynb](GPT4All_SmallLM_Demo.ipynb) notebook and demonstrates how to work with small language models using the **llama-cpp-python** package instead of GPT4All.\n",
    "\n",
    "### What is llama-cpp-python?\n",
    "\n",
    "[llama-cpp-python](https://github.com/abetlen/llama-cpp-python) provides Python bindings for the [llama.cpp](https://github.com/ggml-org/llama.cpp) library, which is a high-performance C++ implementation for running Large Language Models (LLMs) locally. It's one of the most popular libraries for running GGUF models on consumer hardware.\n",
    "\n",
    "### Key Features of llama-cpp-python:\n",
    "\n",
    "1. **Efficient Local Inference**: Runs models entirely on CPU (with optional GPU acceleration)\n",
    "2. **GGUF Model Support**: Works with quantized models in the GGUF format from Hugging Face\n",
    "3. **Low Memory Footprint**: Supports 4-bit, 8-bit, and other quantization levels\n",
    "4. **OpenAI-Compatible API**: Includes a built-in server that mimics OpenAI's API\n",
    "5. **Active Development**: Regular updates to support new model architectures\n",
    "6. **Cross-Platform**: Works on Windows, macOS, and Linux\n",
    "\n",
    "### Why use llama-cpp-python over GPT4All?\n",
    "\n",
    "| Feature | llama-cpp-python | GPT4All |\n",
    "|---------|-----------------|--------|\n",
    "| Model Support | Any GGUF model from Hugging Face | Curated list of ~30 models |\n",
    "| Updates | Very frequent (follows llama.cpp) | Less frequent |\n",
    "| API Style | Direct Python + OpenAI-compatible server | Custom Python API |\n",
    "| Flexibility | More control over inference parameters | Simpler, more abstracted |\n",
    "| Community | Large, active community | Smaller, focused community |\n",
    "\n",
    "**Fun Fact**: GPT4All actually uses llama.cpp as its backend! So this notebook gives you more direct access to the underlying inference engine.\n",
    "\n",
    "### Resources\n",
    "\n",
    "- llama-cpp-python [GitHub Repository](https://github.com/abetlen/llama-cpp-python)\n",
    "- llama-cpp-python [Documentation](https://llama-cpp-python.readthedocs.io/)\n",
    "- llama.cpp [GitHub Repository](https://github.com/ggml-org/llama.cpp)\n",
    "- Hugging Face [GGUF Models](https://huggingface.co/models?search=gguf)\n",
    "\n",
    "### Attribution\n",
    "\n",
    "Notebook originally developed based on work by Greg Merritt <[gmerritt@berkeley.edu](mailto:gmerritt@berkeley.edu)> and adapted by Eric Van Dusen. This llama-cpp-python version was created as an alternative implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "### Installing llama-cpp-python\n",
    "\n",
    "The installation of llama-cpp-python is straightforward but has some important considerations:\n",
    "\n",
    "1. **CPU-only installation** (what we'll use): `pip install llama-cpp-python`\n",
    "2. **GPU acceleration (CUDA)**: Requires building from source with CUDA support\n",
    "3. **Metal acceleration (macOS)**: Automatic on Apple Silicon\n",
    "\n",
    "**Note**: The first time you run inference, the model needs to be loaded into memory. This may take a moment depending on the model size and your hardware.\n",
    "\n",
    "### Steps:\n",
    "1. Ensure that your Python environment has llama-cpp-python installed\n",
    "2. Define the model path where your `.gguf` model files are stored\n",
    "3. Load a model using the `Llama` class\n",
    "\n",
    "_This notebook assumes that at least one 'Small model' file ending in `.gguf` has already been downloaded into a directory (see `GPT4All_Download_gguf.ipynb` for more)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that your python environment has llama-cpp-python capability\n",
    "# Note: This uses the installation pattern specified for this notebook\n",
    "try: \n",
    "    from llama_cpp import Llama\n",
    "except: \n",
    "    %pip install llama-cpp-python\n",
    "    from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Llama Class\n",
    "\n",
    "The `Llama` class is the main interface for loading and interacting with GGUF models. Key parameters include:\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|--------|\n",
    "| `model_path` | Full path to the .gguf model file | Required |\n",
    "| `n_ctx` | Context window size (max tokens model can see) | 512 |\n",
    "| `n_threads` | Number of CPU threads to use | Auto |\n",
    "| `n_gpu_layers` | Layers to offload to GPU (0 = CPU only) | 0 |\n",
    "| `verbose` | Print loading information | True |\n",
    "| `chat_format` | Chat template format (e.g., \"chatml\", \"llama-2\") | Auto-detect |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check out our local filesystem path and whether we have files downloaded\n",
    "\n",
    "We need to locate where our `.gguf` model files are stored. Below are examples for different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1 - if a Shared Hub is being used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This only worked for FA 25 workshop on Cal ICOR Hub\n",
    "#!ls /home/jovyan/shared_readwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Cal-ICOR workshop hub (JupyterCon Nov 2025)\n",
    "!ls /home/jovyan/shared/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2 - if a local machine is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is my local path to a directory called shared-rw\n",
    "!ls shared-rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or the full path ( this is on my laptop) \n",
    "!ls /Users//Users/ericvandusen/Documents/GitHub/shared/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Pick your environment - Local vs Hub - and set the Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model path parameter depending on where you are computing\n",
    "model_directory = \"/home/jovyan/shared/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model path parameter depending on where you are computing\n",
    "#model_directory = \"/Users/ericvandusen/Documents/GitHub/shared/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Loading the Downloaded Model with llama-cpp-python\n",
    "\n",
    "In this step, we create a local instance of the model using the `Llama` class.  \n",
    "\n",
    "**Key differences from GPT4All:**\n",
    "- We provide the **full path** to the model file (not just the filename)\n",
    "- We can specify `n_ctx` (context window size) directly\n",
    "- We have fine-grained control over threading with `n_threads`\n",
    "- The `chat_format` parameter helps the model understand conversation structure\n",
    "\n",
    "**About the model:**\n",
    "`qwen2-1_5b-instruct-q4_0.gguf` is a **1.5 billion-parameter Qwen2 model** that has been **quantized** to reduce its size and memory usage. The `.gguf` extension indicates that the model is stored in the **GGUF format**, which is the standard format for llama.cpp inference.\n",
    "\n",
    "**Note:**  \n",
    "Loading the model may take a few seconds. You'll see verbose output showing the model configuration being loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M4) - 12124 MiB free\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 338 tensors from /Users/ericvandusen/Documents/GitHub/shared/qwen2-1_5b-instruct-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = qwen2-1_5b-instruct\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 1536\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 8960\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 12\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv   8:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = ../Qwen2/gguf/qwen2-1_5b-imatrix/imat...\n",
      "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = ../sft_2406.txt\n",
      "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 196\n",
      "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 1937\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q4_0:  193 tensors\n",
      "llama_model_loader: - type q4_1:    3 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_0\n",
      "print_info: file size   = 888.43 MiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 151643 ('<|endoftext|>')\n",
      "load:   - 151645 ('<|im_end|>')\n",
      "load: special tokens cache size = 293\n",
      "load: token to piece cache size = 0.9338 MB\n",
      "print_info: arch             = qwen2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 1536\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 12\n",
      "print_info: n_head_kv        = 2\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 6\n",
      "print_info: n_embd_k_gqa     = 256\n",
      "print_info: n_embd_v_gqa     = 256\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8960\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = -1\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 1.5B\n",
      "print_info: model params     = 1.54 B\n",
      "print_info: general.name     = qwen2-1_5b-instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 151936\n",
      "print_info: n_merges         = 151387\n",
      "print_info: BOS token        = 151643 '<|endoftext|>'\n",
      "print_info: EOS token        = 151645 '<|im_end|>'\n",
      "print_info: EOT token        = 151645 '<|im_end|>'\n",
      "print_info: PAD token        = 151643 '<|endoftext|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 151643 '<|endoftext|>'\n",
      "print_info: EOG token        = 151645 '<|im_end|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 145 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/29 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =   888.43 MiB\n",
      "load_tensors:   CPU_REPACK model buffer size =   680.70 MiB\n",
      "....repack: repack tensor blk.0.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.1.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.1.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.2.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.2.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.2.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.4.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.4.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.5.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.5.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.8.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.10.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.11.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.11.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.11.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.12.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.13.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.14.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.17.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.18.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.19.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.19.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.20.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_q.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.23.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.23.ffn_down.weight with q4_0_4x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.ffn_gate.weight with q4_0_4x8\n",
      "repack: repack tensor blk.24.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.25.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.25.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.26.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.27.attn_q.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_k.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_v.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.attn_output.weight with q4_0_4x8\n",
      "repack: repack tensor blk.27.ffn_gate.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.27.ffn_down.weight with q4_0_4x8\n",
      ".repack: repack tensor blk.27.ffn_up.weight with q4_0_4x8\n",
      ".\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M4\n",
      "ggml_metal_init: picking default device: Apple M4\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M4\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal4  (5002)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 12713.12 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x12fe30f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x118d5a450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x10fab4990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x10fab58e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x10fab6cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x118d5c160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x12fe31200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x118d5b3e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x10ae38ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x10fab61d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x118d9bf50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x10ae39640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x118d5ab40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x10ae39940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x12fe31500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x10fab70f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x10fab7630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x10ae39e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x10ae3a5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x10fab7930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x118d9d790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x10fab5c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x12fe31800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x118f050f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x10ae3aa60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x10ae3b290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x118f05670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x10fabaa80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x10fab90e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x118f05970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x10fab93e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x118d9eb30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x10faba550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x10fabb290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x118f06950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x10fabb590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x118f05c70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x118f05f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x118f07560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x118f07860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x118d9f670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x118f07b60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x118f08c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x118d9f0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x10ae71da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x118d9fb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x118da09f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x118da0cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x118d9e430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x118f09c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x10fabd0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x118da22e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x118da0090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x118da1a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x118f09f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x10fabdbe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x118f091e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ae72100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x118f0a8a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10fabe1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10fabe4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x10fabeb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ae74230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x118da31c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x118da1d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ae72be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ae73e00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x118da45d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ae73a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ae732c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10fabfc20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x118da3880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ae75030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x118f0c8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x118f0c0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x118da4ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x10ae75330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x118f0c3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x118f0b180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x10fabf830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x10ae75e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x118da51e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x118f0b480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x118f0d0a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x11fe15840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x118f0d3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x118da6540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x118da6a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x10ae76ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x10fac08e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x10fac0be0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x118f0e2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x118f0dca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x118f0e710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x10fac1030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x118da9440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ae76990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x118da8c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10fabc310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x118da8f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x118f10250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x118da8290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x118da9e90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x118daa190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x118daa550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10fac1ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x118daa850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x118daab50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x10ae775e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x104c168c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x118daafc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10fac2f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10fac3230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10fac3530 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x118f118a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x118dabe30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ae78fa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10fac3b30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10fac4150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x118f11e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x118f11380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ae79c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10fac4450 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ae79f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x118dac750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x118daca50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x118dad170 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x118f12c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x118dae710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x118dadeb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x118dae1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x118dad470 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x118f13740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x118daf290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x118daf590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x10fac5750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x118db0150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140814f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ae7b0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ae7b620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ae7bab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x118f14580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10fac49f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10fac7110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ae7c160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ae7c650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x118f14880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x118db14b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ae7c950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x118db19d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10fac8ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x118db1f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ae7cc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ae7d510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ae7d180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x118f14d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10fac8490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x118f16740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x118f16b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ae7f070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x118db3350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x118db38d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x118db2b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ae7e8f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10facaba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x118db2e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x118db2270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x118f17680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10faca640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ae7fa80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x118f17980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ae7fd80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x118f17cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x118db41d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x118f17ff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x118f182f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x118f18960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10facb7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ae80080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10facc410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10faccb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ae80780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x118f193c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x118f19b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10facd580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ae80a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ae80d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ae812d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ae81790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ae81a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x118db58c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ae82080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ae82540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ae829d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10face600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ae82cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x118f1a100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x118f1a400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x118f1ad30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x118db84c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x118db8020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x118db77f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10facecb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x118db7af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x118f1b080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10facefb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10facf2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ae835d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10facf5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10facfe30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x118db88f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10facf8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x118f1b5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x118f1ccc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x118f1dc30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x118dba140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x10fad02d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x10fad0830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x10ae83c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x10ae84310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x10fad1490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x10fad1a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x10fad29b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x118f1d3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x118dbbae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x118f1d6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x118dbbf00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x118dbc200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x10fad3370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x118dbc550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x10ae85db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x118f20790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x10ae84d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x118dbcb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x10ae85020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x118f1ea30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ae86b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x118f1f980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x10fad1d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x118dbd330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x118dbe3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x118dbdfa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x118dbe6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x118dbedb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x118dbe9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x10fad46f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ae86e80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ae873c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x118f1f1d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10fad3db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x118f20cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x10fad5550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x118f20fd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x118dbfe60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x10fad5f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ae88d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x118dc0570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ae879b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x118dc08c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x118f21c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10fad79f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10fad7430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10fad8af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x10ae89900 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x10fad84f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x118f23fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x118f25cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x118f24e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x118dc0e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10fad9080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10fad9380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x118f26cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x118f26250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x118f25120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10fada680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x118dc1510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x118f267a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x118f27d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10fad9b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x118dc1e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x118dc2480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x10fadc2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x118f27120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x118f27830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x118f28410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x118f289b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x118f28f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x118dc4800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10fadd510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ae8af50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x10ae8a460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x10ae8a140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ae88050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x118dc2ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x118dc3b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x118dc4280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x118dc4e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ae8b990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10fade6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x10fadf370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x118dc5130 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10ae8c3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x118f29b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10fadfa70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10fadfea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x118f2a8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10fae07c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x118f2b830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x118dc5a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x118f2bbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10fae0c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x118dc5d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x10fae29d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x118dc6220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x10fae19e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x118dc6520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x10ae8dde0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x10ae8e0e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x118dc6ad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x104c17a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x118f2c690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x14081b4a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x10fae11c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x10fae3220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x118dc80a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x118dc7610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x118f2d040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10fae4000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10ae8e770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10fae45f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x10ae8f070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x10fae48f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x10ae90010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x118f2d390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x10fae54b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x10fae57b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x10fae5ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x118dc95c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x118f2e980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x10fae5db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x118f2ec80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x10fae7b90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10ae8f560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10fae6e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x118f31030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x118dca690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x118dcbbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x118dcc260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x10fae6590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x118f2f1c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x118f30250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x10ae907e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x10fae8a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x10fae8f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x118f313a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x118f318b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x118dcc8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x118dccbd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x118dcced0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x118dcd200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x118f32630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x118f32ab0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10ae8f860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10fae9970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10ae916a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x118dce940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x10ae919a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x118dcd500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x118dce200 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x118f31f10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x10ae92b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x118f331b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x118f33740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x118f33bf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x118f34080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x118dcf0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x118dcf6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x10ae93320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x14081b7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x118f35180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x10faec020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x118dcf9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x118f35600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x14081bd50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x10ae91fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x10ae94140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x118dd01a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x118dd04a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x10ae937a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x118f345c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x118f35f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10ae94a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10ae94e50 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.58 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    56.00 MiB\n",
      "llama_kv_cache_unified: size =   56.00 MiB (  2048 cells,  28 layers,  1/1 seqs), K (f16):   28.00 MiB, V (f16):   28.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 2704\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   299.75 MiB\n",
      "llama_context: graph nodes  = 1070\n",
      "llama_context: graph splits = 64 (with bs=512), 1 (with bs=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Model loaded successfully: qwen2-1_5b-instruct-q4_0.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '196', 'general.quantization_version': '2', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'quantize.imatrix.file': '../Qwen2/gguf/qwen2-1_5b-imatrix/imatrix.dat', 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.pre': 'qwen2', 'quantize.imatrix.chunks_count': '1937', 'tokenizer.ggml.model': 'gpt2', 'general.file_type': '2', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.embedding_length': '1536', 'qwen2.attention.head_count_kv': '2', 'qwen2.context_length': '32768', 'quantize.imatrix.dataset': '../sft_2406.txt', 'qwen2.attention.head_count': '12', 'general.architecture': 'qwen2', 'qwen2.block_count': '28', 'qwen2.feed_forward_length': '8960', 'general.name': 'qwen2-1_5b-instruct'}\n",
      "Available chat formats from metadata: chat_template.default\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the model filename\n",
    "model_name = \"qwen2-1_5b-instruct-q4_0.gguf\"\n",
    "\n",
    "# Create the full path to the model\n",
    "model_path = os.path.join(model_directory, model_name)\n",
    "\n",
    "# Load the model using llama-cpp-python\n",
    "# n_ctx: context window size (how many tokens the model can \"see\" at once)\n",
    "# n_threads: number of CPU threads (None = auto-detect)\n",
    "# verbose: whether to print loading information\n",
    "# chat_format: the chat template format for this model family\n",
    "model = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,          # Context window size\n",
    "    n_threads=None,      # Auto-detect optimal thread count\n",
    "    verbose=True,        # Print model loading info\n",
    "    chat_format=\"chatml\" # Qwen uses ChatML format\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Call the model with a simple user message\n",
    "\n",
    "### Using create_chat_completion()\n",
    "\n",
    "In llama-cpp-python, we use the `create_chat_completion()` method to generate responses. This method follows the OpenAI Chat Completions API format, making it easy to switch between local models and cloud APIs.\n",
    "\n",
    "**Message Structure:**\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"System instructions here\"},\n",
    "    {\"role\": \"user\", \"content\": \"User message here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Previous assistant response (optional)\"}\n",
    "]\n",
    "```\n",
    "\n",
    "**This may take a few moments to process.**\n",
    "\n",
    "You may run this multiple times, and will likely get different results. Feel free to change the `user_message`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    2301.48 ms\n",
      "llama_perf_context_print: prompt eval time =    2299.94 ms /    26 tokens (   88.46 ms per token,    11.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    1369.38 ms /    76 runs   (   18.02 ms per token,    55.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    3698.54 ms /   102 tokens\n",
      "llama_perf_context_print:    graphs reused =         72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "Tariffs on foreign manufactured goods are typically paid by the producer, not the consumer. Tariffs are imposed by governments to protect domestic industries from foreign competition and to ensure that domestic producers can compete fairly. When a tariff is applied to a product, the producer of that product is responsible for paying the tariff, which is then passed on to the consumer in the form of higher prices.\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Who pays for tariffs on foreign manufactured goods? Consumer or Producer?\"  # You can change this prompt\n",
    "\n",
    "# Create the messages list (OpenAI-compatible format)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "# Generate a response using create_chat_completion\n",
    "response = model.create_chat_completion(\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# Extract and print the response\n",
    "print(\"Response:\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Response Object\n",
    "\n",
    "The `create_chat_completion()` method returns a dictionary that follows the OpenAI API response format:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"id\": \"chatcmpl-...\",\n",
    "    \"object\": \"chat.completion\",\n",
    "    \"created\": 1234567890,\n",
    "    \"model\": \"model_name\",\n",
    "    \"choices\": [\n",
    "        {\n",
    "            \"index\": 0,\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": \"The actual response text...\"\n",
    "            },\n",
    "            \"finish_reason\": \"stop\"\n",
    "        }\n",
    "    ],\n",
    "    \"usage\": {\n",
    "        \"prompt_tokens\": 10,\n",
    "        \"completion_tokens\": 50,\n",
    "        \"total_tokens\": 60\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "To get the text content, we access: `response[\"choices\"][0][\"message\"][\"content\"]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Passing additional arguments to control generation\n",
    "\n",
    "The `create_chat_completion()` method accepts many parameters to control the generation:\n",
    "\n",
    "| Parameter | Description | Default |\n",
    "|-----------|-------------|--------|\n",
    "| `messages` | List of message dictionaries | Required |\n",
    "| `max_tokens` | Maximum number of tokens to generate | 16 |\n",
    "| `temperature` | Controls randomness (0 = deterministic, higher = more random) | 0.8 |\n",
    "| `top_p` | Nucleus sampling (consider tokens with top_p cumulative probability) | 0.95 |\n",
    "| `top_k` | Only consider the top_k most likely tokens | 40 |\n",
    "| `repeat_penalty` | Penalize repeated tokens (1.0 = no penalty) | 1.1 |\n",
    "| `stream` | If True, returns a generator for streaming responses | False |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Using the `max_tokens` argument to cap the length of the response\n",
    "\n",
    "Generation will stop abruptly once it reaches the maximum number of tokens, even if the response is mid-sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 8 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2301.48 ms\n",
      "llama_perf_context_print: prompt eval time =     120.28 ms /    17 tokens (    7.08 ms per token,   141.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =     723.83 ms /    59 runs   (   12.27 ms per token,    81.51 tokens per second)\n",
      "llama_perf_context_print:       total time =     864.62 ms /    76 tokens\n",
      "llama_perf_context_print:    graphs reused =         56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "The economic outcome of tariffs on foreign manufactured goods is complex and can vary depending on several factors, including the nature of the tariffs, the industries affected, and the overall economic conditions of the country. Here are some potential economic outcomes:\n",
      "\n",
      "1. **Increased Revenue**: Tariffs can generate revenue for the government\n"
     ]
    }
   ],
   "source": [
    "response_size_limit_in_tokens = 60  # You can change this parameter\n",
    "\n",
    "user_message = \"What is the economic outcome of tariffs on foreign manufactured goods?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "response = model.create_chat_completion(\n",
    "    messages=messages,\n",
    "    max_tokens=response_size_limit_in_tokens\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. The `temperature` argument\n",
    "\n",
    "LLMs generate one token (\"word\") at a time as they complete the text. At each step, there's a probability distribution over possible next tokens. The **temperature** parameter controls how this distribution is sampled:\n",
    "\n",
    "- **`temperature = 0`** (\"cold\"): Always picks the most likely token → deterministic output\n",
    "- **`temperature = 0.5-0.7`** (\"warm\"): Balanced creativity and coherence\n",
    "- **`temperature = 1.0`** (\"hot\"): High variety but may be less coherent\n",
    "- **`temperature > 1.0`** (\"very hot\"): Very random, often incoherent\n",
    "\n",
    "**Let's run the same prompt three times with `temperature = 0`; we expect identical outputs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 8 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2301.48 ms\n",
      "llama_perf_context_print: prompt eval time =     278.49 ms /    15 tokens (   18.57 ms per token,    53.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =     383.66 ms /    29 runs   (   13.23 ms per token,    75.59 tokens per second)\n",
      "llama_perf_context_print:       total time =     672.90 ms /    44 tokens\n",
      "llama_perf_context_print:    graphs reused =         27\n",
      "Llama.generate: 22 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tariffs can have a significant impact on the prices of foreign manufactured goods. When a country imposes tariffs on imported goods, it means that the country is\n",
      "\n",
      "Response 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    2301.48 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     393.07 ms /    30 runs   (   13.10 ms per token,    76.32 tokens per second)\n",
      "llama_perf_context_print:       total time =     402.25 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         28\n",
      "Llama.generate: 22 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tariffs can have a significant impact on the prices of foreign manufactured goods. When a country imposes tariffs on imported goods, it means that the country is\n",
      "\n",
      "Response 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    2301.48 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     393.77 ms /    30 runs   (   13.13 ms per token,    76.19 tokens per second)\n",
      "llama_perf_context_print:       total time =     403.09 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tariffs can have a significant impact on the prices of foreign manufactured goods. When a country imposes tariffs on imported goods, it means that the country is\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_size_limit_in_tokens = 30\n",
    "number_of_responses = 3\n",
    "temperature = 0.0  # You can change this parameter\n",
    "\n",
    "user_message = \"How will tariffs affect the prices of foreign manufactured goods\"\n",
    "\n",
    "for i in range(number_of_responses):\n",
    "    print(f\"Response {i + 1}:\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    response = model.create_chat_completion(\n",
    "        messages=messages,\n",
    "        max_tokens=response_size_limit_in_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    print(f\"{response['choices'][0]['message']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's repeat with a slightly \"hotter\" temperature of `temperature = 0.25`; we expect outputs to begin diverging:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 12 prefix-match hit, remaining 6 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2301.48 ms\n",
      "llama_perf_context_print: prompt eval time =     410.04 ms /     6 tokens (   68.34 ms per token,    14.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =     411.59 ms /    29 runs   (   14.19 ms per token,    70.46 tokens per second)\n",
      "llama_perf_context_print:       total time =     832.59 ms /    35 tokens\n",
      "llama_perf_context_print:    graphs reused =         27\n",
      "Llama.generate: 17 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tariffs can have a significant impact on the political landscape, particularly in countries where trade plays a significant role in their economy. Here are some ways in\n",
      "\n",
      "Response 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    2301.48 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     429.85 ms /    30 runs   (   14.33 ms per token,    69.79 tokens per second)\n",
      "llama_perf_context_print:       total time =     439.86 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         28\n",
      "Llama.generate: 17 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tariffs can have a significant impact on elections, as they can affect the cost of goods and services, which can influence voter preferences. If tariffs increase\n",
      "\n",
      "Response 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    2301.48 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     391.54 ms /    30 runs   (   13.05 ms per token,    76.62 tokens per second)\n",
      "llama_perf_context_print:       total time =     402.55 ms /    31 tokens\n",
      "llama_perf_context_print:    graphs reused =         28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tariffs can have a significant impact on the economy and, in turn, on the outcome of elections. Here are some ways in which tariffs might affect\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response_size_limit_in_tokens = 30\n",
    "number_of_responses = 3\n",
    "temperature = 0.25\n",
    "\n",
    "user_message = \"How will tariffs affect elections\"\n",
    "\n",
    "for i in range(number_of_responses):\n",
    "    print(f\"Response {i + 1}:\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    response = model.create_chat_completion(\n",
    "        messages=messages,\n",
    "        max_tokens=response_size_limit_in_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    print(f\"{response['choices'][0]['message']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A \"very hot\" temperature of `temperature = 1` will result in high variety but may lead to less satisfactory responses:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_size_limit_in_tokens = 30\n",
    "number_of_responses = 5\n",
    "temperature = 1\n",
    "\n",
    "user_message = \"How will tariffs affect elections\"\n",
    "\n",
    "for i in range(number_of_responses):\n",
    "    print(f\"Response {i + 1}:\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    \n",
    "    response = model.create_chat_completion(\n",
    "        messages=messages,\n",
    "        max_tokens=response_size_limit_in_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    print(f\"{response['choices'][0]['message']['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Include a hidden \"system message\" at the start of the conversation\n",
    "\n",
    "A **system message** sets the context and personality for the assistant. It's placed at the beginning of the messages list and influences how the model responds to all subsequent user messages.\n",
    "\n",
    "In llama-cpp-python, we simply include a message with `\"role\": \"system\"` as the first item in the messages list:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Your system instructions here...\"},\n",
    "    {\"role\": \"user\", \"content\": \"User's question\"}\n",
    "]\n",
    "```\n",
    "\n",
    "**Note:** System messages are never guaranteed to remain secret; models can sometimes be prompted to reveal their instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_size_limit_in_tokens = 100\n",
    "\n",
    "system_message = \"\"\"\n",
    "You are a hard working economics student at UC Berkeley. \n",
    "You think that there may be some truth to the things you learn in economics classes.\n",
    "You wish that the people in the government understood economics.\n",
    "You think that memes and poems and pop songs are a good way to communicate\n",
    "Answer in rap lyrics always\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"How will tariffs affect inflation\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "response = model.create_chat_completion(\n",
    "    messages=messages,\n",
    "    max_tokens=response_size_limit_in_tokens\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. \"Few-shot\" learning: include conversation history to set the tone\n",
    "\n",
    "**Few-shot learning** involves providing example prompt/response pairs to establish a pattern. The model will statistically tend to follow this pattern when generating new responses.\n",
    "\n",
    "In llama-cpp-python, this is elegantly handled by simply including multiple `user`/`assistant` message pairs before the actual user query:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"System prompt\"},\n",
    "    {\"role\": \"user\", \"content\": \"Example question 1\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Example answer 1\"},\n",
    "    {\"role\": \"user\", \"content\": \"Example question 2\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Example answer 2\"},\n",
    "    {\"role\": \"user\", \"content\": \"Actual user question\"}  # Real question\n",
    "]\n",
    "```\n",
    "\n",
    "The model learns the response style from the examples and applies it to the new question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. A \"Few-shot\" example\n",
    "\n",
    "In this example, we establish a pattern of concise, informative responses about economics. The model should follow this established conversational style.\n",
    "\n",
    "**Note:** We're using the native message format which llama-cpp-python automatically converts to the appropriate chat template (ChatML for Qwen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_size_limit_in_tokens = 200\n",
    "\n",
    "# System message sets the overall behavior\n",
    "system_message = \"\"\"\n",
    "You are an economics tutor with a focus on international trade.\n",
    "Answer concisely and clearly, using accessible language.\n",
    "\"\"\"\n",
    "\n",
    "# Few-shot examples establish the response pattern\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    # Example 1\n",
    "    {\"role\": \"user\", \"content\": \"What is a tariff?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A tariff is a tax imposed by a government on imported goods, often used to protect domestic industries.\"},\n",
    "    # Example 2\n",
    "    {\"role\": \"user\", \"content\": \"How do tariffs affect consumer prices?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Tariffs typically raise the price of imported goods, making them more expensive for consumers.\"},\n",
    "    # Example 3\n",
    "    {\"role\": \"user\", \"content\": \"Can tariffs backfire?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Yes, they can lead to trade wars, hurt exporters, and reduce overall economic efficiency.\"},\n",
    "    # Example 4\n",
    "    {\"role\": \"user\", \"content\": \"How do other countries respond to tariffs?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"They often retaliate with their own tariffs, targeting key export sectors.\"},\n",
    "    # The actual user question\n",
    "    {\"role\": \"user\", \"content\": \"What is an example of a real-world tariff dispute?\"}\n",
    "]\n",
    "\n",
    "# Generate response\n",
    "response = model.create_chat_completion(\n",
    "    messages=messages,\n",
    "    max_tokens=response_size_limit_in_tokens,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(\"Response:\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. The importance of proper chat formatting\n",
    "\n",
    "One advantage of llama-cpp-python is that it **automatically handles chat templating** when you set the `chat_format` parameter (we set it to `\"chatml\"` for Qwen models).\n",
    "\n",
    "Behind the scenes, your messages are converted to special tokens like:\n",
    "```\n",
    "<|im_start|>system\n",
    "You are an economics tutor...\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "What is a tariff?\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "```\n",
    "\n",
    "If you used the wrong format (or no format), the model might:\n",
    "- Continue writing the script rather than responding as an assistant\n",
    "- Produce incoherent output\n",
    "- Not follow the conversation structure\n",
    "\n",
    "**The `chat_format` parameter ensures proper formatting automatically!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. A note about \"hallucinations\"\n",
    "\n",
    "It's popular to use the word \"hallucinations\" to talk about model output that is very different from what we wanted, or when the output does not seem to make sense.\n",
    "\n",
    "However, an LLM does not perceive; it merely predicts the most likely next token based on patterns in its training data. The term \"hallucination\" can be misleading because:\n",
    "\n",
    "1. **The model isn't failing** — it's doing exactly what it's designed to do\n",
    "2. **It's a statistical process** — sometimes low-probability completions happen\n",
    "3. **Training data limitations** — the model can only draw from what it learned\n",
    "\n",
    "Understanding this helps us have realistic expectations and design better prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d. Building a chatbot application\n",
    "\n",
    "If you wanted to build an extended conversation experience, you would:\n",
    "\n",
    "1. **Maintain a message history list** that grows with each exchange\n",
    "2. **Append new user messages** to the history\n",
    "3. **Append assistant responses** to the history after generation\n",
    "4. **Pass the entire history** to each new call\n",
    "\n",
    "```python\n",
    "# Example chatbot loop structure\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    response = model.create_chat_completion(messages=messages)\n",
    "    assistant_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "    print(f\"Assistant: {assistant_message}\")\n",
    "```\n",
    "\n",
    "**Important:** The LLM itself has no \"memory\" — it's your application that stores and manages the conversation history. Each call processes the entire conversation from the beginning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bonus: Streaming responses\n",
    "\n",
    "llama-cpp-python supports **streaming**, which lets you see tokens as they're generated (like ChatGPT). This is useful for:\n",
    "- Better user experience (immediate feedback)\n",
    "- Long responses (no waiting for complete generation)\n",
    "- Real-time applications\n",
    "\n",
    "Set `stream=True` to enable streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"Explain the concept of comparative advantage in international trade.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "print(\"Response (streaming):\")\n",
    "\n",
    "# With streaming, we get a generator that yields chunks\n",
    "stream = model.create_chat_completion(\n",
    "    messages=messages,\n",
    "    max_tokens=150,\n",
    "    stream=True  # Enable streaming\n",
    ")\n",
    "\n",
    "# Print each chunk as it arrives\n",
    "for chunk in stream:\n",
    "    delta = chunk[\"choices\"][0][\"delta\"]\n",
    "    if \"content\" in delta:\n",
    "        print(delta[\"content\"], end=\"\", flush=True)\n",
    "\n",
    "print()  # New line at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "\n",
    "1. **Install and import** llama-cpp-python\n",
    "2. **Load a GGUF model** using the `Llama` class\n",
    "3. **Generate responses** using `create_chat_completion()`\n",
    "4. **Control generation** with parameters like `max_tokens`, `temperature`, `top_p`\n",
    "5. **Use system messages** to set assistant behavior\n",
    "6. **Implement few-shot learning** with example conversations\n",
    "7. **Stream responses** for real-time output\n",
    "\n",
    "### Key Advantages of llama-cpp-python:\n",
    "- **OpenAI-compatible API** — easy to switch between local and cloud models\n",
    "- **Any GGUF model** — not limited to a curated list\n",
    "- **Active development** — regular updates for new model architectures\n",
    "- **Fine-grained control** — access to low-level inference parameters\n",
    "\n",
    "### Next Steps:\n",
    "- Try different GGUF models from Hugging Face\n",
    "- Experiment with different temperature and sampling settings\n",
    "- Build a simple chatbot application\n",
    "- Explore GPU acceleration for faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
