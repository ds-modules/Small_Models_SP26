{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01895865-587f-4f01-8944-28bc0f45a65b",
   "metadata": {},
   "source": [
    "# Chatbot - Gradio X llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e40798-a2a6-4013-9367-5a69607e1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from IPython.display import IFrame, display\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d7eaae-a2d9-42dc-a1e9-797b3ac2dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86baaae-db8b-4c21-ad07-443e04f8b1f9",
   "metadata": {},
   "source": [
    "## Running a Gradio App on JupyterHub\n",
    "\n",
    "This code demonstrates how to run a **Gradio app** (a simple web-based interface for Python functions) inside a **JupyterHub environment** such as DataHub or CloudBank.\n",
    "\n",
    "Gradio normally launches on `localhost`, but on JupyterHub the server runs behind a proxy ‚Äî so we use the environment variable `JUPYTERHUB_SERVICE_PREFIX` to route the app correctly through the proxy system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721b773-b8f0-4b4b-9298-b48481a835c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo1 = gr.Interface(fn=lambda x: f\"Hello {x}!\", inputs=\"text\", outputs=\"text\")\n",
    "\n",
    "base_url = os.environ.get('JUPYTERHUB_SERVICE_PREFIX', '/')\n",
    "demo1.launch(\n",
    "    share=False,\n",
    "    prevent_thread_lock=True,\n",
    "    server_port=7860,\n",
    "    root_path=f\"{base_url}proxy/7860\",\n",
    "    inline=False\n",
    ")\n",
    "\n",
    "proxy_url = f\"{base_url}proxy/7860/\"\n",
    "display(IFrame(src=proxy_url, width=1000, height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9025f4-15f2-47c8-ab30-b8b404b9e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05ef4c-a998-4b40-9f57-9a1f4b5c4e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ce40e7-0023-49c7-be39-f4fa304cda40",
   "metadata": {},
   "source": [
    "## Set up the llama-cpp-python framework\n",
    "\n",
    "and Run llama-cpp-python behind the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f3734-fab3-412e-8fd0-a0d6bf80ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/jovyan/shared/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e217df11-8fd0-4ee9-adf5-7da8bb47f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Llama(\n",
    "    model_path=os.path.join(path, \"qwen2-1_5b-instruct-q4_0.gguf\"),\n",
    "    n_ctx=2048,\n",
    "    n_threads=None,\n",
    "    verbose=True,\n",
    "    chat_format=\"chatml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac3aa2-c71c-4a65-9c93-0688f990561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to call the model\n",
    "def chat_with_model(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = model.create_chat_completion(messages=messages, max_tokens=256)\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Define Gradio interface\n",
    "demo2 = gr.Interface(fn=chat_with_model, inputs=\"text\", outputs=\"text\", title=\"Small Model Chat\")\n",
    "\n",
    "# Launch on JupyterHub proxy\n",
    "base_url = os.environ.get('JUPYTERHUB_SERVICE_PREFIX', '/')\n",
    "demo2.launch(\n",
    "    share=False,\n",
    "    prevent_thread_lock=True,\n",
    "    server_port=7860,\n",
    "    root_path=f\"{base_url}proxy/7860\",\n",
    "    inline=False\n",
    ")\n",
    "\n",
    "# Display inline in the notebook\n",
    "proxy_url = f\"{base_url}proxy/7860/\"\n",
    "display(IFrame(src=proxy_url, width=1000, height=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca20420f-b648-45fd-948f-8966bcacf706",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db249207-20cf-42ef-98b1-8049aac5d918",
   "metadata": {},
   "source": [
    "## Lets Build the History of the Chat \n",
    "\n",
    "This will be a Json file that stores the chat history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff361a-1eb5-48ed-bbd0-3fb73afaeeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d43958-8549-40c7-995e-356f399c8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Persistent History File =====\n",
    "HISTORY_FILE = \"chat_history.json\"\n",
    "\n",
    "def load_history():\n",
    "    if os.path.exists(HISTORY_FILE):\n",
    "        try:\n",
    "            with open(HISTORY_FILE, \"r\") as f:\n",
    "                content = f.read().strip()\n",
    "                if not content:\n",
    "                    return []  # empty file\n",
    "                return json.loads(content)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"‚ö†Ô∏è Warning: history file is corrupted or empty, resetting it.\")\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def save_history(history):\n",
    "    with open(HISTORY_FILE, \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "history = load_history()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178cdb89-89d1-4fe7-b0e5-c55e29d12ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Chat Function =====\n",
    "def chat_with_model(user_input):\n",
    "    \"\"\"Append user input to history, generate response, and persist conversation.\"\"\"\n",
    "    # Reload history each time in case file changed externally\n",
    "    global history\n",
    "    history = load_history()\n",
    "\n",
    "    # Build messages list from history\n",
    "    messages = []\n",
    "    for h in history:\n",
    "        messages.append({\"role\": \"user\", \"content\": h['user']})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": h['model']})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Generate model response\n",
    "    response = model.create_chat_completion(messages=messages, max_tokens=100)\n",
    "    response_text = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # Update and persist history\n",
    "    history.append({\"user\": user_input, \"model\": response_text})\n",
    "    save_history(history)\n",
    "\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce9f516-4524-4f36-ae56-f63e7ef9ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Gradio Interface =====\n",
    "demo3 = gr.Interface(\n",
    "    fn=chat_with_model,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Persistent Small Model Chat\",\n",
    "    description=\"Chat with a local llama-cpp-python model that remembers previous conversations.\"\n",
    ")\n",
    "\n",
    "\n",
    "base_url = os.environ.get('JUPYTERHUB_SERVICE_PREFIX', '/')\n",
    "demo3.launch(\n",
    "    share=False,\n",
    "    prevent_thread_lock=True,\n",
    "    server_port=7860,\n",
    "    root_path=f\"{base_url}proxy/7860\",\n",
    "    inline=False\n",
    ")\n",
    "\n",
    "proxy_url = f\"{base_url}proxy/7860/\"\n",
    "display(IFrame(src=proxy_url, width=1000, height=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a4f4e-a1d1-4b42-a26f-e6879e98220e",
   "metadata": {},
   "source": [
    "## Now we can check the history that we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab34d5d-1689-4231-92ef-a9ad3ca4dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for turn in json.load(open(\"chat_history.json\")):\n",
    "    print(f\"üßë User: {turn['user']}\\nü§ñ Model: {turn['model']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c07204-47fb-40f4-8bf9-76ad8b35a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to clear the history \n",
    "#open(\"chat_history.json\", \"w\").write(\"[]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3034ed77-9062-442c-a7cf-797869e478f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc1e2b-66bb-4709-a7c5-57f702373b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95daee4c-ed93-41f8-9aca-c7d781b29e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
